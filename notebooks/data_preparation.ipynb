{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Keyframe metadata\n",
    "combined_keyframes_metadata_filename = None\n",
    "keyframes_metadata_dir = None\n",
    "\n",
    "# Object extraction metadata\n",
    "combined_object_extraction_filename = None\n",
    "object_extraction_dir = None\n",
    "\n",
    "# OCR metadata\n",
    "combined_ocr_metadata_filename = None\n",
    "ocr_metadata_dir = None\n",
    "\n",
    "# Tag metadata\n",
    "combined_tag_metadata_filename = None\n",
    "tag_metadata_dir = None\n",
    "\n",
    "# Audio metadata\n",
    "combined_audio_metadata_filename = None\n",
    "audio_metadata_dir = None\n",
    "\n",
    "image_dir = None\n",
    "# # Final metadata\n",
    "# final_metadata_filename = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_extraction_path = f'{dir_path}/data_extraction'\n",
    "dataset_path = f'{data_extraction_path}/dataset/AIC_Video'\n",
    "\n",
    "# Keyframe metadata\n",
    "if not combined_keyframes_metadata_filename:\n",
    "    combined_keyframes_metadata_filename = 'keyframes_metadata.json'\n",
    "    \n",
    "if not keyframes_metadata_dir:\n",
    "    keyframes_metadata_dir = f'{data_extraction_path}/transnet/keyframes_metadata'\n",
    "    \n",
    "# Object extraction metadata\n",
    "if not combined_object_extraction_filename:\n",
    "    combined_object_extraction_filename = 'object_extraction_metadata.json'\n",
    "    \n",
    "if not object_extraction_dir:\n",
    "    object_extraction_dir = f'{data_extraction_path}/metadata/object_extraction/object_detection'\n",
    "    \n",
    "# OCR metadata\n",
    "if not combined_ocr_metadata_filename:\n",
    "    combined_ocr_metadata_filename = 'ocr_metadata.json'\n",
    "    \n",
    "if not ocr_metadata_dir:\n",
    "    ocr_metadata_dir = f'{data_extraction_path}/metadata/ocr'\n",
    "    \n",
    "# Tag metadata\n",
    "if not combined_tag_metadata_filename:\n",
    "    combined_tag_metadata_filename = 'tag_metadata.json'\n",
    "    \n",
    "if not tag_metadata_dir:\n",
    "    tag_metadata_dir = f'{data_extraction_path}/metadata/tag'\n",
    "    \n",
    "# Audio metadata\n",
    "if not combined_audio_metadata_filename:\n",
    "    combined_audio_metadata_filename = 'audio_metadata.json'\n",
    "    \n",
    "if not audio_metadata_dir:\n",
    "    audio_metadata_dir = f'{data_extraction_path}/audio/audio_recognition'\n",
    "\n",
    "if not image_dir:\n",
    "    image_dir = f'{data_extraction_path}/transnet'\n",
    "       \n",
    "# # final_metadata\n",
    "# if not final_metadata_filename:\n",
    "#     final_metadata_filename = 'final_metadata.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_size(image_path):\n",
    "    \"\"\"\n",
    "    Detects the width and height of an image using OpenCV.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): The path to the image file.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (width, height) of the image.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is not None:\n",
    "        height, width = img.shape[:2]\n",
    "        return width, height\n",
    "    else:\n",
    "        raise ValueError(\"Could not load image from the provided path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_path_from_dir(dir, ext='.json'):\n",
    "    all_paths = {}\n",
    "\n",
    "    for part in sorted(os.listdir(dir)):\n",
    "        data_part_path = os.path.join(dir, part)\n",
    "\n",
    "        if os.path.isdir(data_part_path):\n",
    "            # e.g: handle 'od/L01_V001' structure (nested folder)\n",
    "            data_part = os.path.basename(part)\n",
    "            all_paths[data_part] = []\n",
    "\n",
    "            files = sorted(glob.glob(os.path.join(data_part_path, f'*{ext}')))\n",
    "            all_paths[data_part] = files\n",
    "        else:\n",
    "            # e.g: handle flatten file 'ocr/L01_V001_000_ocr.json'\n",
    "            data_part = os.path.splitext(os.path.basename(part))[0]\n",
    "            all_paths[data_part] = []\n",
    "\n",
    "            if part.endswith(ext):\n",
    "                all_paths[data_part].append(data_part_path)\n",
    "\n",
    "    return all_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine keyframe metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing frame in L01_V001.json: 100%|██████████| 272/272 [00:03<00:00, 80.69it/s]\n",
      "processing frame in L01_V001_extra.json: 100%|██████████| 933/933 [00:04<00:00, 212.12it/s]\n",
      "processing frame in L01_V002.json: 100%|██████████| 216/216 [00:02<00:00, 92.64it/s]\n",
      "processing frame in L01_V002_extra.json: 100%|██████████| 711/711 [00:03<00:00, 227.90it/s]\n",
      "processing data folder: 100%|██████████| 4/4 [00:13<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined keyframe metadata successful: keyframes_metadata.json - total: 2132\n"
     ]
    }
   ],
   "source": [
    "def combine_keyframe_metadata_json_files(directory, image_dir, output_file):\n",
    "    combined_data = {}\n",
    "    all_file_path = parse_file_path_from_dir(directory)\n",
    "            \n",
    "    for file_paths in tqdm(all_file_path.values(), desc=\"processing data folder\"):\n",
    "        file_path = file_paths[0]\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "            \n",
    "        for frame_data in tqdm(data.values(), desc=f\"processing frame in {file_path.split('/')[-1]}\"):\n",
    "            image_path = f\"{image_dir}/{frame_data['frame_path']}\"\n",
    "            img_width, img_height = get_image_size(image_path)\n",
    "            frame_data['width'] = img_width\n",
    "            frame_data['height'] = img_height\n",
    "            \n",
    "        combined_data.update(data)\n",
    "\n",
    "    sorted_combined_data = {\n",
    "        key: combined_data[key] for key in sorted(combined_data)}\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(sorted_combined_data, outfile)\n",
    "    \n",
    "    print(\n",
    "        f'Combined keyframe metadata successful: {output_file} - total: {len(sorted_combined_data)}')\n",
    "    \n",
    "\n",
    "combine_keyframe_metadata_json_files(keyframes_metadata_dir, image_dir, combined_keyframes_metadata_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine object extraction metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing file in L01_V001: 100%|██████████| 203/203 [00:00<00:00, 16243.35it/s]\n",
      "processing file in L01_V001_extra: 100%|██████████| 595/595 [00:00<00:00, 22822.02it/s]\n",
      "processing file in L01_V002: 100%|██████████| 159/159 [00:00<00:00, 21231.91it/s]\n",
      "processing file in L01_V002_extra: 100%|██████████| 420/420 [00:00<00:00, 8129.25it/s]\n",
      "processing video folder: 100%|██████████| 4/4 [00:00<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined object detection metadata successful: object_extraction_metadata.json - total: 1377\n"
     ]
    }
   ],
   "source": [
    "def preprocess_object_detection(data):\n",
    "    organized_data = {'objects': dict(), 'counts': dict()}\n",
    "    for item in data:\n",
    "        label = item['label']\n",
    "        if label not in organized_data['objects']:\n",
    "            organized_data['objects'][label] = []\n",
    "            organized_data['counts'][label] = 0\n",
    "        organized_data['objects'][label].append({\n",
    "            \"score\": item['score'],\n",
    "            \"box\": item['box']\n",
    "        })\n",
    "        organized_data['counts'][label] += 1\n",
    "\n",
    "    return organized_data\n",
    "\n",
    "def combine_object_extraction_metadata_json_files(directory, output_file):\n",
    "    combined_data = {}\n",
    "    all_file_path = parse_file_path_from_dir(directory)\n",
    "\n",
    "    for video_folder, file_paths in tqdm(all_file_path.items(), desc=\"processing video folder\"):\n",
    "        # video_name_part = video_folder.split('_')  # L01, V001, extra\n",
    "        # video_id = '_'.join(video_name_part[:2]) # L01_V001\n",
    "        # is_extra = '_extra' if len(video_name_part) == 3 else ''\n",
    "        for file_path in tqdm(file_paths, desc=f\"processing file in {video_folder}\"):\n",
    "            file = file_path.split('/')[-1] # 000139_detection.json, etc.\n",
    "            file_name = file.split('.')[0].split('_')  # [000139, detection]\n",
    "            file_id, file_model = file_name[0], file_name[1]\n",
    "            key = f'{video_folder}_{int(file_id):06d}_{file_model}'\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            if not data:\n",
    "                continue\n",
    "            combined_data[key] = preprocess_object_detection(data)\n",
    "                \n",
    "    sorted_combined_data = {\n",
    "        key: combined_data[key] for key in sorted(combined_data)}\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(sorted_combined_data, f)\n",
    "\n",
    "    print(f'Combined object detection metadata successful: {output_file} - total: {len(sorted_combined_data)}')\n",
    "    \n",
    "\n",
    "combine_object_extraction_metadata_json_files(object_extraction_dir, combined_object_extraction_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing data in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/notebooks/data_extraction/metadata/ocr/L01_V001.json: 100%|██████████| 266/266 [00:00<00:00, 440459.88it/s]\n",
      "processing data in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/notebooks/data_extraction/metadata/ocr/L01_V001_extra.json: 100%|██████████| 829/829 [00:00<00:00, 985287.05it/s]\n",
      "processing data in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/notebooks/data_extraction/metadata/ocr/L01_V002.json: 100%|██████████| 213/213 [00:00<00:00, 283344.99it/s]\n",
      "processing data in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/notebooks/data_extraction/metadata/ocr/L01_V002_extra.json: 100%|██████████| 614/614 [00:00<00:00, 783242.90it/s]\n",
      "processing data folder: 100%|██████████| 4/4 [00:00<00:00, 209.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined ocr metadata successful: ocr_metadata.json - total: 1922\n"
     ]
    }
   ],
   "source": [
    "keyframe_metadata_file = f\"{dir_path}/keyframes_metadata.json\"\n",
    "\n",
    "def combine_ocr_metadata_json_file(directory, output_file):\n",
    "    combined_data = {}\n",
    "    all_file_path = parse_file_path_from_dir(directory)\n",
    "    \n",
    "    for file_paths in tqdm(all_file_path.values(), desc=\"processing data folder\"):\n",
    "        file_path = file_paths[0]\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        if not data:\n",
    "            continue\n",
    "\n",
    "        # e.g ocr/L01_V001_extra.json => L01_V001_extra\n",
    "        video_name = file_path.split('/')[-1].split('.')[0]\n",
    "        \n",
    "        for frame_img, ocr_data in tqdm(data.items(), desc=f\"processing data in {file_path}\"):\n",
    "            frame_id = frame_img.split('.')[0] # 001.jpg => 001\n",
    "            key = f\"{video_name}_{int(frame_id):06d}_ocr\"\n",
    "            combined_data[key] = ocr_data\n",
    "            \n",
    "    sorted_combined_data = {\n",
    "        key: combined_data[key] for key in sorted(combined_data)}\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(sorted_combined_data, f)\n",
    "        \n",
    "    print(\n",
    "        f'Combined ocr metadata successful: {output_file} - total: {len(sorted_combined_data)}')\n",
    "    \n",
    "combine_ocr_metadata_json_file(ocr_metadata_dir, combined_ocr_metadata_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing data in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/notebooks/data_extraction/metadata/tag/L01_V001.json: 100%|██████████| 272/272 [00:00<00:00, 917082.55it/s]\n",
      "processing data in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/notebooks/data_extraction/metadata/tag/L01_V001_extra.json: 100%|██████████| 933/933 [00:00<00:00, 461146.08it/s]\n",
      "processing data in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/notebooks/data_extraction/metadata/tag/L01_V002.json: 100%|██████████| 216/216 [00:00<00:00, 589824.00it/s]\n",
      "processing data in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/notebooks/data_extraction/metadata/tag/L01_V002_extra.json: 100%|██████████| 711/711 [00:00<00:00, 1136490.15it/s]\n",
      "processing data folder: 100%|██████████| 4/4 [00:00<00:00, 210.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined multi tag metadata successful: tag_metadata.json - total: 2132\n"
     ]
    }
   ],
   "source": [
    "keyframe_metadata_file = f\"{dir_path}/keyframes_metadata.json\"\n",
    "\n",
    "def combine_multi_tag_metadata_json_file(directory, output_file):\n",
    "    combined_data = {}\n",
    "    all_file_path = parse_file_path_from_dir(directory)\n",
    "    \n",
    "    for file_paths in tqdm(all_file_path.values(), desc=\"processing data folder\"):\n",
    "        file_path = file_paths[0]\n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        if not data:\n",
    "            continue\n",
    "        \n",
    "        # e.g tag/L01_V001_extra.json => L01_V001_extra\n",
    "        video_name = file_path.split('/')[-1].split('.')[0]\n",
    "\n",
    "        for frame_img, tag_data in tqdm(data.items(), desc=f\"processing data in {file_path}\"):\n",
    "            frame_id = frame_img.split('.')[0]  # 001.jpg => 001\n",
    "            key = f\"{video_name}_{int(frame_id):06d}_tag\"\n",
    "            combined_data[key] = tag_data\n",
    "    \n",
    "    sorted_combined_data = {\n",
    "        key: combined_data[key] for key in sorted(combined_data)}\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(sorted_combined_data, f)\n",
    "\n",
    "    print(\n",
    "        f'Combined multi tag metadata successful: {output_file} - total: {len(sorted_combined_data)}')\n",
    "    \n",
    "combine_multi_tag_metadata_json_file(tag_metadata_dir, combined_tag_metadata_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyframe_metadata_file = f\"{dir_path}/keyframes_metadata.json\"\n",
    "\n",
    "# def combine_audio_tag_metadata_json_file(audio_directory, output_file, keyframe_metadata_file):\n",
    "#     combined_data = {}\n",
    "\n",
    "#     if os.path.exists(output_file):\n",
    "#         with open(output_file, 'r') as existing_file:\n",
    "#             combined_data = json.load(existing_file)\n",
    "\n",
    "#     with open(keyframe_metadata_file, \"r\") as keyframe_file:\n",
    "#         keyframe_metadata_dict = json.load(keyframe_file)\n",
    "        \n",
    "#     for sub_dir in (os.listdir(audio_directory)):\n",
    "#         sub_dir_path = f'{audio_directory}/{sub_dir}'\n",
    "#         for audio_json in (os.listdir(sub_dir_path)):\n",
    "#             video_id = f\"{sub_dir}_{audio_json.split('.')[0]}\"\n",
    "#             audio_json_path = f\"{sub_dir_path}/{audio_json}\"\n",
    "#             with open(audio_json_path, \"r\") as audio_file:\n",
    "#                 audio_list = json.load(audio_file)\n",
    "#             for item in audio_list:\n",
    "#                 for keyframe_key, keyframe_value in keyframe_metadata_dict.items():\n",
    "#                     frame_idx = keyframe_key.split(\"_\")[-1]\n",
    "#                     start, end = item['segment_id']\n",
    "#                     video_id_keyframe = keyframe_value['video_path'].split(\".\")[0].split('/')[-1]\n",
    "#                     if (video_id == video_id_keyframe) and (int(frame_idx) >= start) and (int(frame_idx) <= end):\n",
    "#                         combined_data[f\"{keyframe_key}_audio\"] = item['transcription']\n",
    "                        \n",
    "                        \n",
    "#     fill_empty_audio_combined_data = {}\n",
    "#     keys_combined_data = combined_data.keys() # Taking all keys in combined data dict\n",
    "#     for key_keyframe in keyframe_metadata_dict.keys():\n",
    "#         if f\"{key_keyframe}_audio\" not in keys_combined_data:\n",
    "#             fill_empty_audio_combined_data[f\"{key_keyframe}_audio\"] = {\"transcription\" : \"\"}\n",
    "#     combined_data.update(fill_empty_audio_combined_data)\n",
    "    \n",
    "    \n",
    "#     keys = list(combined_data.keys())\n",
    "#     keys.sort()\n",
    "#     sorted_combined_data = {i : combined_data[i] for i in keys}\n",
    "#     with open(output_file, 'w') as outfile:\n",
    "#         json.dump(sorted_combined_data, outfile)\n",
    "    \n",
    "# combine_audio_tag_metadata_json_file(audio_metadata_dir, combined_audio_metadata_filename, keyframe_metadata_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined final file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def combined_json_file(json_files, output_file):\n",
    "#     combined_data = {}\n",
    "    \n",
    "#     for file_name, data in json_files.items():\n",
    "#         path = data['path']\n",
    "#         key_ext = data.get('key_extension', '')\n",
    "#         with open(path, 'r') as f:\n",
    "#             json_data = json.load(f)\n",
    "#         print(f\"Preprocessing metadata file: {file_name}\")\n",
    "#         for key, data in json_data.items():\n",
    "#             if key_ext:\n",
    "#                 key = key.replace(f'_{key_ext}', '')\n",
    "#             if key not in combined_data:\n",
    "#                 combined_data[key] = {}\n",
    "                \n",
    "#             combined_data[key][key_ext] = data\n",
    "            \n",
    "#     with open(output_file, 'w') as f:\n",
    "#         json.dump(combined_data, f)\n",
    "\n",
    "#     print(f'Combined final metadata successful: {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_files = {\n",
    "#     'keyframe_metadata': {\n",
    "#         'path': combined_keyframes_metadata_filename,\n",
    "#         'key_extension': 'keyframe',\n",
    "#     },\n",
    "#     'object_extraction': {\n",
    "#         'path': combined_object_extraction_filename,\n",
    "#         'key_extension': 'detection'\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # combined_json_file(json_files, final_metadata_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
