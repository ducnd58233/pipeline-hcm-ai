{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "save_dir = None\n",
    "checkpoint_dir = None\n",
    "keyframes_dir = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/xinyu1205/recognize-anything.git\n",
    "%pip install timm transformers fairscale pycocoevalcap\n",
    "\n",
    "# change the working directory to the cloned repo\n",
    "%cd recognize-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = os.getcwd() # Get the current directory path\n",
    "parent_path = os.path.dirname(os.path.dirname(dir_path))\n",
    "\n",
    "if save_dir is None:\n",
    "    save_dir = f\"{parent_path}/metadata/tag_output\"\n",
    "if checkpoint_dir is None:\n",
    "    checkpoint_dir = f\"{parent_path}/metadata/pretrained\"\n",
    "if keyframes_dir is None:\n",
    "    keyframes_dir = f'{parent_path}/transnet/Keyframes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from ram.models import ram_plus\n",
    "from ram import get_transform\n",
    "from tqdm import tqdm\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download CheckPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_checkpoints(model, checkpoint_dir):\n",
    "    print('You selected', model)\n",
    "    print(f'Checkpoint directory: {checkpoint_dir}')\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    print(checkpoint_dir)\n",
    "\n",
    "    if model == \"RAM++\":\n",
    "        ram_weights_path = os.path.join(checkpoint_dir, 'ram_plus_swin_large_14m.pth')\n",
    "        if not os.path.exists(ram_weights_path):\n",
    "            url = \"https://huggingface.co/xinyu1205/recognize-anything-plus-model/resolve/main/ram_plus_swin_large_14m.pth\"\n",
    "            subprocess.run(['wget', url, '-O', os.path.join(checkpoint_dir, 'ram_plus_swin_large_14m.pth')], check=True)\n",
    "            print(f\"RAM weights downloaded to {ram_weights_path}\")\n",
    "        else:\n",
    "            print(\"RAM weights already downloaded!\")\n",
    "\n",
    "model = \"RAM++\"\n",
    "download_checkpoints(model, checkpoint_dir)\n",
    "print(model, 'weights are downloaded!')\n",
    "print(keyframes_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_data_path(keyframes_dir):\n",
    "    all_keyframe_paths = dict()\n",
    "    for part in sorted(os.listdir(keyframes_dir)):\n",
    "        all_keyframe_paths[part] =  dict() # L01, L02\n",
    "\n",
    "    for data_part in sorted(all_keyframe_paths.keys()):\n",
    "        data_part_path = f'{keyframes_dir}/{data_part}'       \n",
    "        video_dirs = sorted(os.listdir(data_part_path))         # ['V001', 'V002', ...]\n",
    "        video_ids = [video_dir for video_dir in video_dirs] \n",
    "        for video_id, video_dir in zip(video_ids, video_dirs):\n",
    "            keyframe_paths = sorted(glob.glob(f'{data_part_path}/{video_dir}/*.jpg'))\n",
    "            all_keyframe_paths[data_part][video_id] = keyframe_paths\n",
    "\n",
    "    return all_keyframe_paths\n",
    "    \n",
    "all_keyframe_paths = parse_data_path(keyframes_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from PIL import Image\n",
    "from ram.models import ram_plus\n",
    "from ram import inference_ram as inference\n",
    "from ram import get_transform\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# Configuration\n",
    "PRETRAINED_MODEL_PATH = f'{checkpoint_dir}/ram_plus_swin_large_14m.pth'\n",
    "IMAGE_SIZE = 384\n",
    "\n",
    "def main():\n",
    "    def create_directory(path):\n",
    "        \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    transform = get_transform(image_size=IMAGE_SIZE)\n",
    "\n",
    "    # Load model\n",
    "    model = ram_plus(pretrained=PRETRAINED_MODEL_PATH, image_size=IMAGE_SIZE, vit='swin_l')\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "\n",
    "    # Initialize output dictionary\n",
    "    output_data = {}\n",
    "\n",
    "    # Process each keyframe\n",
    "    for part in tqdm(all_keyframe_paths, desc=\"Processing parts\"):\n",
    "        for video_id in tqdm(all_keyframe_paths[part], desc=f\"Processing videos in {part}\", leave=False):\n",
    "            for image_path in tqdm(all_keyframe_paths[part][video_id], desc=f\"Processing keyframes in {video_id}\", leave=False):\n",
    "                try:\n",
    "                    image = transform(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "                    res = inference(image, model)\n",
    "                    tags = res[0].split(' | ')\n",
    "                    output_data[os.path.basename(image_path)] = tags\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {str(e)}\")   \n",
    "                feature_type = \"tagging\"\n",
    "                # Save output JSON\n",
    "                json_path = os.path.join(\n",
    "                    save_dir, f'{part}', f'{video_id}_{feature_type}.json')\n",
    "                create_directory(os.path.dirname(json_path))\n",
    "                with open(json_path, 'w') as f:\n",
    "                    json.dump(output_data, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the cloned repo\n",
    "rm -rf ../recognize-anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the pretrained weights\n",
    "rm -rf ../pretrained"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcmcai-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
