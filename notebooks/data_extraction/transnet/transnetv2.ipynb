{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "videos_dir = None\n",
    "save_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "if not videos_dir:\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        # Update this path as necessary\n",
    "        videos_dir = f'{dir_path}/AIC_Video'\n",
    "    elif 'kaggle' in str(get_ipython()):\n",
    "        videos_dir = f'{dir_path}/AIC_Video'\n",
    "    else:\n",
    "        parent_dir_path = os.path.dirname(dir_path)\n",
    "        videos_dir = f'{parent_dir_path}/dataset/AIC_Video'\n",
    "        \n",
    "if not save_dir:\n",
    "    save_dir = f'{dir_path}/scene_JSON'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TransNetV2'...\n",
      "remote: Enumerating objects: 362, done.\u001b[K\n",
      "remote: Counting objects: 100% (84/84), done.\u001b[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
      "remote: Total 362 (delta 70), reused 70 (delta 70), pack-reused 278 (from 1)\u001b[K\n",
      "Receiving objects: 100% (362/362), 95.27 KiB | 1.07 MiB/s, done.\n",
      "Resolving deltas: 100% (210/210), done.\n",
      "Filtering content: 100% (3/3), 34.77 MiB | 8.01 MiB/s, done.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "fetch: Fetching reference refs/heads/feat/test\n",
      "Checking out LFS objects: 100% (6/6), 0 B | 0 B/s, done.                        \n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/soCzech/TransNetV2.git\n",
    "!git lfs install\n",
    "!cd TransNetV2\n",
    "!git lfs fetch https://github.com/soCzech/TransNetV2.git\n",
    "!git lfs checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 00:28:02.885138: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-12 00:28:02.886673: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-12 00:28:02.916225: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-12 00:28:03.398027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import module\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import glob\n",
    "import ffmpeg\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from TransNetV2.inference.transnetv2 import TransNetV2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse video info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_video_info(videos_dir):\n",
    "    \"\"\"Parse video information from the directory structure.\"\"\"\n",
    "    all_video_paths = {}\n",
    "    for part in sorted(os.listdir(videos_dir)):\n",
    "        data_part = part.split('_')[-1]  # L01, L02 for example\n",
    "        all_video_paths[data_part] = {}\n",
    "\n",
    "    for data_part in sorted(all_video_paths.keys()):\n",
    "        data_part_path = f'{videos_dir}/Videos_{data_part}/video'\n",
    "        video_paths = sorted(os.listdir(data_part_path))\n",
    "        video_ids = [video_path.replace('.mp4', '').split('_')[-1] for video_path in video_paths]\n",
    "        for video_id, video_path in zip(video_ids, video_paths):\n",
    "            video_path_full = f'{data_part_path}/{video_path}'\n",
    "            all_video_paths[data_part][video_id] = video_path_full\n",
    "\n",
    "    return all_video_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(path):\n",
    "    \"\"\"Create a directory if it does not exist.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def predict_scenes_and_save(model, all_video_paths, save_dir):\n",
    "    \"\"\"Predict scenes for each video and save the results as JSON files.\"\"\"\n",
    "    create_directory(save_dir)\n",
    "\n",
    "    for key, video_paths_dict in tqdm(all_video_paths.items()):      \n",
    "        video_ids = sorted(video_paths_dict.keys())\n",
    "        key_dir = os.path.join(save_dir, key)\n",
    "        create_directory(key_dir)\n",
    "\n",
    "        for video_id in tqdm(video_ids, desc=f\"Processing {key}\"):            \n",
    "            video_path = video_paths_dict[video_id]\n",
    "            _, single_frame_predictions, _ = model.predict_video(video_path)\n",
    "\n",
    "            # Generate list of scenes from predictions, returns tuples of (start frame, end frame)\n",
    "            scenes = model.predictions_to_scenes(single_frame_predictions)\n",
    "\n",
    "            with open(os.path.join(key_dir, f\"{video_id}.json\"), 'w') as f:\n",
    "                json.dump(scenes.tolist(), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Using weights from /home/heigatvu/my-project/competition/pipeline-hcm-ai/notebooks/data_extraction/transnet/TransNetV2/inference/transnetv2-weights/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 00:28:04.446183: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-12 00:28:04.446689: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Extracting frames from /home/heigatvu/my-project/competition/pipeline-hcm-ai/notebooks/data_extraction/dataset/AIC_Video/Videos_L01/video/L01_V001.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 00:28:21.648203: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'inputs' with dtype float and shape [1,100,27,48,3]\n",
      "\t [[{{node inputs}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Processing video frames 31665/31665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Extracting frames from /home/heigatvu/my-project/competition/pipeline-hcm-ai/notebooks/data_extraction/dataset/AIC_Video/Videos_L01/video/L01_V002.mp4\n",
      "[TransNetV2] Processing video frames 24300/24337"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing L01: 100%|██████████| 2/2 [03:44<00:00, 112.27s/it]\n",
      "100%|██████████| 1/1 [03:44<00:00, 224.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TransNetV2] Processing video frames 24337/24337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = TransNetV2()\n",
    "all_video_paths = parse_video_info(videos_dir)\n",
    "predict_scenes_and_save(model, all_video_paths, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
