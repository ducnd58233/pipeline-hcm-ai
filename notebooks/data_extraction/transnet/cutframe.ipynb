{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "videos_dir = None\n",
    "scene_json_dirs = None\n",
    "save_dir_all = None\n",
    "metadata_dir_all = None\n",
    "extract_metadata_only = None\n",
    "# Change this to the desired number of frames per segment\n",
    "num_frames_per_segment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "\n",
    "\n",
    "if not videos_dir:\n",
    "    if 'google.colab' in str(get_ipython()):\n",
    "        # Update this path as necessary\n",
    "        videos_dir = f'{dir_path}/AIC_Video'\n",
    "    elif 'kaggle' in str(get_ipython()):\n",
    "        videos_dir = f'{dir_path}/AIC_Video'\n",
    "    else:\n",
    "        parent_dir_path = os.path.dirname(dir_path)\n",
    "        videos_dir = f'{parent_dir_path}/dataset/AIC_Video'\n",
    "    \n",
    "if not scene_json_dirs:\n",
    "    scene_json_dirs = f'{dir_path}/scene_JSON'\n",
    "    \n",
    "if not save_dir_all:\n",
    "    save_dir_all = f'{dir_path}/keyframes'\n",
    "    \n",
    "if not metadata_dir_all:\n",
    "    metadata_dir_all = f'{dir_path}/keyframes_metadata'\n",
    "    \n",
    "if not extract_metadata_only:\n",
    "    extract_metadata_only = False\n",
    "    \n",
    "if not num_frames_per_segment:\n",
    "    num_frames_per_segment = 3  # Change this to the desired number of frames per segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'TransNetV2' already exists and is not an empty directory.\n",
      "Updated Git hooks.\n",
      "Git LFS initialized.\n",
      "fetch: Fetching reference refs/heads/feat/test\n",
      "Checking out LFS objects: 100% (6/6), 0 B | 0 B/s, done.                        \n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/soCzech/TransNetV2.git\n",
    "!git lfs install\n",
    "!cd TransNetV2\n",
    "!git lfs fetch https://github.com/soCzech/TransNetV2.git\n",
    "!git lfs checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_python(obj):\n",
    "    \"\"\"Convert numpy types to standard Python types.\"\"\"\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "\n",
    "def parse_video_info(videos_dir):\n",
    "    \"\"\"Parse video information from the directory structure.\"\"\"\n",
    "    all_video_paths = {}\n",
    "    for part in sorted(os.listdir(videos_dir)):\n",
    "        data_part = part.split('_')[-1]\n",
    "        all_video_paths[data_part] = {}\n",
    "\n",
    "    for data_part in sorted(all_video_paths.keys()):\n",
    "        data_part_path = os.path.join(\n",
    "            videos_dir, f'Videos_{data_part}', 'video')\n",
    "        video_paths = sorted(os.listdir(data_part_path))\n",
    "        video_ids = [video_path.replace('.mp4', '').split(\n",
    "            '_')[-1] for video_path in video_paths]\n",
    "        for video_id, video_path in zip(video_ids, video_paths):\n",
    "            video_path_full = os.path.join(data_part_path, video_path)\n",
    "            all_video_paths[data_part][video_id] = video_path_full\n",
    "\n",
    "    return all_video_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process cutting frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evenly_spaced_frames(start_idx, end_idx, num_frames):\n",
    "    \"\"\"Get evenly spaced frame indices between start_idx and end_idx.\"\"\"\n",
    "    return np.linspace(start_idx, end_idx, num_frames, dtype=int)\n",
    "\n",
    "\n",
    "def get_frame_timestamp(frame_index, fps):\n",
    "    \"\"\"Convert frame index to timestamp in seconds.\"\"\"\n",
    "    return frame_index / fps\n",
    "\n",
    "\n",
    "def create_directory(directory):\n",
    "    \"\"\"Create a directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "def get_relative_path(path, base_dir):\n",
    "    \"\"\"Get the relative path from base_dir to path.\"\"\"\n",
    "    return os.path.relpath(path, base_dir)\n",
    "\n",
    "\n",
    "def get_video_fps(video_path):\n",
    "    \"\"\"Get the fps of a video file.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cap.release()\n",
    "    return fps\n",
    "\n",
    "def process_metadata(video_path, scene_json_path, save_dir, videos_dir, fps, num_frames_per_segment):\n",
    "    \"\"\"Process and extract metadata from video segments.\"\"\"\n",
    "    with open(scene_json_path, 'r') as f:\n",
    "        video_scenes = json.load(f)\n",
    "\n",
    "    frame_metadata = {}\n",
    "    video_id = os.path.basename(video_path).split('.')[0]\n",
    "\n",
    "    relative_video_path = get_relative_path(video_path, videos_dir)\n",
    "    keyframes_dir = os.path.dirname(os.path.dirname(save_dir))\n",
    "\n",
    "    for i, (start, end) in enumerate(video_scenes):\n",
    "        frame_indices = get_evenly_spaced_frames(\n",
    "            start, end, num_frames_per_segment)\n",
    "\n",
    "        for index in frame_indices:\n",
    "            timestamp = get_frame_timestamp(index, fps)\n",
    "            filename = f\"{index:06d}.jpg\"\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            relative_frame_path = get_relative_path(filepath, keyframes_dir)\n",
    "\n",
    "            frame_metadata[f\"{video_id}_extra_{index:06d}\"] = {\n",
    "                \"shot_index\": numpy_to_python(i),\n",
    "                \"frame_index\": numpy_to_python(index),\n",
    "                \"shot_start\": numpy_to_python(start),\n",
    "                \"shot_end\": numpy_to_python(end),\n",
    "                \"timestamp\": numpy_to_python(timestamp),\n",
    "                \"video_path\": relative_video_path,\n",
    "                \"frame_path\": relative_frame_path\n",
    "            }\n",
    "\n",
    "    return frame_metadata\n",
    "\n",
    "\n",
    "def sample_frames(video_path, scene_json_path, save_dir, videos_dir, fps, num_frames_per_segment, width=None, height=None):\n",
    "    \"\"\"Sample frames from video segments, resize them, and save them to the specified directory.\"\"\"\n",
    "    create_directory(save_dir)\n",
    "\n",
    "    frame_metadata = process_metadata(\n",
    "        video_path, scene_json_path, save_dir, videos_dir, fps, num_frames_per_segment)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    for frame_id, metadata in frame_metadata.items():\n",
    "        index = metadata['frame_index']\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, index)\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            # Resize the frame if width and height are provided\n",
    "            if width is not None and height is not None:\n",
    "                frame = cv2.resize(frame, (width, height))\n",
    "            \n",
    "            filename = f\"{index:06d}.jpg\"\n",
    "            filepath = os.path.join(save_dir, filename)\n",
    "            if not cv2.imwrite(filepath, frame):\n",
    "                print('Failed to save frame:', filepath)\n",
    "        else:\n",
    "            print('Failed to read frame at index:', index)\n",
    "    \n",
    "    cap.release()\n",
    "    return frame_metadata\n",
    "\n",
    "def process_videos(all_video_paths, scene_json_dirs, save_dir_all, metadata_dir_all, videos_dir, num_frames_per_segment, extract_only=False, width= 640, height= 640):\n",
    "    \"\"\"Process all videos: sample frames (if needed) and extract metadata.\"\"\"\n",
    "    create_directory(save_dir_all)\n",
    "    create_directory(metadata_dir_all)\n",
    "\n",
    "    for key in all_video_paths.keys():\n",
    "        # save_dir = os.path.join(save_dir_all, key)\n",
    "\n",
    "        # if not all_video_paths[key]:\n",
    "        #     print(f\"Skipping empty AIC_Video subdirectory: {key}\")\n",
    "        #     continue\n",
    "\n",
    "        scene_json_subdir = os.path.join(scene_json_dirs, key)\n",
    "        if not os.path.exists(scene_json_subdir) or not os.listdir(scene_json_subdir):\n",
    "            print(\n",
    "                f\"Skipping empty or non-existent Scene_JSON subdirectory: {key}\")\n",
    "            continue\n",
    "\n",
    "        # create_directory(save_dir)\n",
    "\n",
    "        video_paths_dict = all_video_paths[key]\n",
    "        video_ids = sorted(video_paths_dict.keys())\n",
    "\n",
    "        # Calculate fps once for this group of videos\n",
    "        first_video_path = video_paths_dict[video_ids[0]]\n",
    "        fps = get_video_fps(first_video_path)\n",
    "\n",
    "        for video_id in tqdm(video_ids, desc=f\"Processing videos in {key}\"):\n",
    "            video_path = video_paths_dict[video_id]\n",
    "            video_scene_path = os.path.join(\n",
    "                scene_json_dirs, key, f\"{video_id}.json\")\n",
    "\n",
    "            if not os.path.exists(video_scene_path):\n",
    "                print(f\"Skipping missing Scene_JSON file: {video_scene_path}\")\n",
    "                continue\n",
    "\n",
    "            save_dir_video = os.path.join(save_dir_all, f\"{key}_{video_id}_extra\")\n",
    "\n",
    "            if extract_only:\n",
    "                frame_metadata = process_metadata(\n",
    "                    video_path, video_scene_path, save_dir_video, videos_dir, fps, num_frames_per_segment)\n",
    "            else:\n",
    "                frame_metadata = sample_frames(\n",
    "                    video_path, video_scene_path, save_dir_video, videos_dir, fps, num_frames_per_segment, width= width, height= height)\n",
    "\n",
    "            # Save metadata for this video\n",
    "            metadata_filename = os.path.join(\n",
    "                metadata_dir_all, f\"{key}_{video_id}_extra.json\")\n",
    "            with open(metadata_filename, 'w') as f:\n",
    "                json.dump(frame_metadata, f, default=numpy_to_python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing videos in L01:   0%|          | 0/2 [00:00<?, ?it/s][h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d312db00] mmco: unref short failure\n",
      "Processing videos in L01:  50%|█████     | 1/2 [00:31<00:31, 31.10s/it][h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "[h264 @ 0x5ef7d311bd00] mmco: unref short failure\n",
      "Processing videos in L01: 100%|██████████| 2/2 [00:56<00:00, 28.11s/it]\n"
     ]
    }
   ],
   "source": [
    "all_video_paths = parse_video_info(videos_dir)\n",
    "process_videos(all_video_paths, scene_json_dirs, save_dir_all,\n",
    "               metadata_dir_all, videos_dir, num_frames_per_segment, extract_only=False, width= 640, height= 640)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create metadata keyframe for organizing committee (option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyframes_path = f'{save_dir_all}'\n",
    "organizing_committee_name_dir = os.listdir(keyframes_path)\n",
    "organizing_committee_keyframe_paths = [f\"{keyframes_path}/{name}\" for name in organizing_committee_name_dir if not name.endswith('extra')]\n",
    "\n",
    "for organizing_committee_keyframe_path in organizing_committee_keyframe_paths:\n",
    "    \n",
    "    video_path = f\"{videos_dir}/Videos_{organizing_committee_keyframe_path.split('/')[-1].split('_')[0]}/video/{organizing_committee_keyframe_path.split('/')[-1]}.mp4\"\n",
    "    scene_json_path = f\"{scene_json_dirs}/{organizing_committee_keyframe_path.split('/')[-1].split('_')[0]}/{organizing_committee_keyframe_path.split('/')[-1].split('_')[1]}.json\"\n",
    "    csv_file_path = f\"{dir_path}/map-keyframes/{organizing_committee_keyframe_path.split('/')[-1]}.csv\"\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    with open(scene_json_path, 'r') as f:\n",
    "        video_scenes = json.load(f)\n",
    "    \n",
    "    json_data = {}\n",
    "    # Iterate through each row in the dataframe to populate the JSON structure\n",
    "    for index, row in df.iterrows():\n",
    "        frame_index = int(row['frame_idx'])  # Assuming your CSV has a column 'frame_index'\n",
    "        \n",
    "        for idx, (start, end) in enumerate(video_scenes):\n",
    "            if (frame_index >= start) and (frame_index <= end):\n",
    "                key = f\"{organizing_committee_keyframe_path.split('/')[-1]}_{frame_index:06d}\"  # Create the key with zero-padded frame index\n",
    "                json_data[key] = {\n",
    "                    \"shot_index\": idx,  # Based on its index in scene_JSON\n",
    "                    \"frame_index\": frame_index,\n",
    "                    \"shot_start\": start,  # Based on its start of shot in range of scene_JSON\n",
    "                    \"shot_end\": end,     # Based on its end of shot in range of scene_JSON\n",
    "                    \"timestamp\": float(row['pts_time']),\n",
    "                    \"video_path\": get_relative_path(video_path, videos_dir),\n",
    "                    \"frame_path\": get_relative_path(f\"{organizing_committee_keyframe_path}/{int(row['n']):03d}.jpg\", os.path.dirname(keyframes_path))\n",
    "                }\n",
    "\n",
    "                # Convert the dictionary to a JSON string\n",
    "                json_output = json.dumps(json_data)\n",
    "\n",
    "                # Save the JSON to a file\n",
    "                json_output_path = f\"{dir_path}/keyframes_metadata/{organizing_committee_keyframe_path.split('/')[-1]}.json\"\n",
    "                with open(json_output_path, 'w') as json_file:\n",
    "                    json_file.write(json_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rename following structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_keys(json_file):\n",
    "    # Read the JSON file\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # New dictionary to store renamed keys\n",
    "    new_data = {}\n",
    "    \n",
    "    # Regular expression to match the key pattern\n",
    "    pattern = r'(L\\d+_V\\d+)_(\\d+)_(extra)'\n",
    "    \n",
    "    # Rename keys\n",
    "    for key, value in data.items():\n",
    "        if re.match(pattern, key):\n",
    "            new_key = re.sub(pattern, r'\\1_\\3_\\2', key)\n",
    "            new_data[new_key] = value\n",
    "        else:\n",
    "            new_data[key] = value  # Keep keys that don't match the pattern unchanged\n",
    "    \n",
    "    # Write the updated data back to the JSON file\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(new_data, f, indent=2)\n",
    "        \n",
    "        \n",
    "def process_folder(folder_path):\n",
    "    # Iterate through all files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('_extra.json'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            rename_keys(file_path)\n",
    "            print(f\"Completed processing: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: L01_V001_extra.json\n",
      "Completed processing: L01_V001_extra.json\n",
      "Processing file: L01_V002_extra.json\n",
      "Completed processing: L01_V002_extra.json\n",
      "Keys renamed successfully.\n"
     ]
    }
   ],
   "source": [
    "process_folder(metadata_dir_all)\n",
    "print(\"Keys renamed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
