{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "metadata_path = None\n",
    "save_dir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import save_npz, load_npz, vstack, csr_matrix\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.getcwd()\n",
    "parent_dir_path = os.path.dirname(dir_path)\n",
    "\n",
    "if not metadata_path:\n",
    "    metadata_path = f'{parent_dir_path}/final_metadata.json'\n",
    "    \n",
    "if not save_dir:\n",
    "    save_dir = f'{dir_path}/metadata_encoded'\n",
    "    \n",
    "os.makedirs(save_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_metadata(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "metadata = load_metadata(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared data for 46996 frames\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(metadata):\n",
    "    object_texts = []\n",
    "    ocr_texts = []\n",
    "    tag_texts = []\n",
    "    frame_ids = []\n",
    "    \n",
    "    for frame_id, frame_data in metadata.items():\n",
    "        if 'detection' in frame_data and 'objects' in frame_data['detection']:\n",
    "            objects = frame_data['detection']['objects']\n",
    "            counts = frame_data['detection']['counts']\n",
    "            object_text = ' '.join([f\"{obj} \" * counts[obj] for obj in objects.keys()])\n",
    "            object_texts.append(object_text)\n",
    "        else:\n",
    "            object_texts.append('')\n",
    "            \n",
    "        ocr_texts.append(frame_data.get('ocr', ''))\n",
    "        tag_texts.append(' '.join(frame_data.get('tags', [])))\n",
    "        frame_ids.append(frame_id)\n",
    "    \n",
    "    return ocr_texts, object_texts, tag_texts, frame_ids\n",
    "\n",
    "ocr_texts, object_texts, tag_texts, frame_ids = prepare_data(metadata)\n",
    "print(f\"Prepared data for {len(frame_ids)} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* vectorizer.pkl:\n",
    "    * contains vocabulary and IDF (Inverse Document Frequency)\n",
    "    * Example\n",
    "```\n",
    "{\n",
    "    'vocabulary_': {'person': 0, 'tie': 1, 'car': 2, ...},\n",
    "    'idf_': [1.2, 1.5, 1.8, ...],\n",
    "    ...\n",
    "}\n",
    "```\n",
    "* vectors.npz\n",
    "    * contains vector feature of TF-IDF in each frames\n",
    "    * Example (when convert to dense matrix)\n",
    "```\n",
    "[\n",
    "  [0.5, 0.8, 0.0, ...],\n",
    "  [0.0, 0.3, 0.6, ...],\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unable to create vectors for ocr. Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Ocr vectorization complete. Shape: (46996, 0)\n",
      "Object vectorization complete. Shape: (46996, 90)\n",
      "Warning: Unable to create vectors for tag. Error: empty vocabulary; perhaps the documents only contain stop words\n",
      "Tag vectorization complete. Shape: (46996, 0)\n"
     ]
    }
   ],
   "source": [
    "def create_tfidf_vectors(texts, name, output_dir):\n",
    "    vectorizer = TfidfVectorizer(min_df=1)\n",
    "    try:\n",
    "        vectors = vectorizer.fit_transform(texts)\n",
    "    except ValueError as e:\n",
    "        print(f\"Warning: Unable to create vectors for {name}. Error: {str(e)}\")\n",
    "        vectorizer = None\n",
    "        vectors = csr_matrix((len(texts), 0))\n",
    "\n",
    "    if vectorizer is not None:\n",
    "        with open(os.path.join(output_dir, f'{name}_vectorizer.pkl'), 'wb') as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "\n",
    "    save_npz(os.path.join(output_dir, f'{name}_vectors.npz'), vectors)\n",
    "\n",
    "    print(f\"{name.capitalize()} vectorization complete. Shape: {vectors.shape}\")\n",
    "\n",
    "    return vectorizer, vectors\n",
    "\n",
    "ocr_vectorizer, ocr_vectors = create_tfidf_vectors(ocr_texts, 'ocr', save_dir)\n",
    "object_vectorizer, object_vectors = create_tfidf_vectors(\n",
    "    object_texts, 'object', save_dir)\n",
    "tag_vectorizer, tag_vectors = create_tfidf_vectors(tag_texts, 'tag', save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 frames similar to 'person tie':\n",
      "Frame ID: L01_V005_023928, Similarity: 1.0000000000000002\n",
      "Frame ID: L01_V003_004664, Similarity: 1.0000000000000002\n",
      "Frame ID: L01_V008_027233, Similarity: 1.0000000000000002\n",
      "Frame ID: L01_V008_026447, Similarity: 1.0000000000000002\n",
      "Frame ID: L01_V008_026428, Similarity: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "def search_similar_frames(query, vectorizers, vectors, frame_ids, top_k=5):\n",
    "    similarities = []\n",
    "    for vectorizer, vector in zip(vectorizers, vectors):\n",
    "        if vectorizer:\n",
    "            query_vector = vectorizer.transform([query])\n",
    "            similarity = vector.dot(query_vector.T).toarray().flatten()\n",
    "            similarities.append(similarity)\n",
    "\n",
    "    if similarities:\n",
    "        total_similarity = np.sum(similarities, axis=0)\n",
    "        top_indices = total_similarity.argsort()[-top_k:][::-1]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'frame_id': frame_ids[idx],\n",
    "                'similarity': total_similarity[idx]\n",
    "            })\n",
    "\n",
    "        return results\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "\n",
    "query = \"person tie\"\n",
    "results = search_similar_frames(\n",
    "    query,\n",
    "    [object_vectorizer, ocr_vectorizer, tag_vectorizer],\n",
    "    [object_vectors, ocr_vectors, tag_vectors],\n",
    "    frame_ids\n",
    ")\n",
    "\n",
    "print(f\"Top 5 frames similar to '{query}':\")\n",
    "for result in results:\n",
    "    print(\n",
    "        f\"Frame ID: {result['frame_id']}, Similarity: {result['similarity']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
