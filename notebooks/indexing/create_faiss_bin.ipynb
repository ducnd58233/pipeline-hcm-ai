{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "feature_shape = None\n",
    "features_dir = None\n",
    "cpu_bin_name = None\n",
    "gpu_bin_name = None\n",
    "ocr_bin_name = None\n",
    "multi_tag_bin_name = None\n",
    "metadata_encoded_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = os.getcwd()\n",
    "parent_dir_path = os.path.dirname(dir_path)\n",
    "\n",
    "if not feature_shape:\n",
    "    feature_shape = 512\n",
    "    \n",
    "if not features_dir:\n",
    "    features_dir = f'{parent_dir_path}/data_extraction/clip/CLIPv2_features'\n",
    "\n",
    "if not cpu_bin_name:\n",
    "    cpu_bin_name = 'faiss_clipv2_cosine_cpu.bin'\n",
    "    \n",
    "if not gpu_bin_name:\n",
    "    gpu_bin_name = 'faiss_clipv2_cosine_gpu.bin'\n",
    "\n",
    "if not ocr_bin_name:\n",
    "    ocr_bin_name = \"faiss_ocr_cosine.bin\"\n",
    "    \n",
    "if not metadata_encoded_path:\n",
    "    metadata_encoded_path = f\"{dir_path}/metadata_encoded\"\n",
    "    \n",
    "if not ocr_bin_name:\n",
    "    ocr_bin_name = \"faiss_ocr_cosine.bin\"\n",
    "if not multi_tag_bin_name:\n",
    "    multi_tag_bin_name = \"faiss_multi_tag_cosine.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/venv/lib/python3.9/site-packages (1.8.0.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.0 in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/venv/lib/python3.9/site-packages (from faiss-cpu) (1.23.3)\n",
      "Requirement already satisfied: packaging in /home/jiggle/personal/competition/hcm-ai/pipeline-hcm-ai/venv/lib/python3.9/site-packages (from faiss-cpu) (24.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def search_similar_frames(query, index, vectorizer, frame_ids, top_k=5):\n",
    "#     \"\"\"\n",
    "#     Search for similar frames using FAISS index.\n",
    "\n",
    "#     Parameters:\n",
    "#     - query: The query text\n",
    "#     - index: The FAISS index\n",
    "#     - vectorizer: The vectorizer to transform the query text\n",
    "#     - frame_ids: List of frame IDs corresponding to the vectors in the index\n",
    "#     - top_k: Number of top results to return\n",
    "\n",
    "#     Returns:\n",
    "#     - List of dictionaries containing frame_id and similarity score\n",
    "#     \"\"\"\n",
    "#     # Transform the query using the vectorizer\n",
    "#     query_vector = vectorizer.embed(query)\n",
    "\n",
    "#     # Perform the search\n",
    "#     distances, indices = index.search(query_vector, top_k)\n",
    "\n",
    "#     # Prepare the results\n",
    "#     results = []\n",
    "#     for i, idx in enumerate(indices[0]):\n",
    "#         results.append({\n",
    "#             'frame_id': frame_ids[idx],\n",
    "#             # Convert distance to similarity\n",
    "#             'similarity': 1 - distances[0][i],\n",
    "#         })\n",
    "\n",
    "#     return results\n",
    "\n",
    "\n",
    "# def visualize_search_results(query, results, visual_encoding, metadata, image_dir):\n",
    "#     print(f\"\\nTop {len(results)} frames similar to query '{query}':\")\n",
    "\n",
    "#     n_images = len(results)\n",
    "#     n_cols = min(3, n_images)\n",
    "#     n_rows = math.ceil(n_images / n_cols)\n",
    "\n",
    "#     fig, axs = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 5*n_rows))\n",
    "#     if n_images == 1:\n",
    "#         axs = np.array([axs])\n",
    "#     axs = axs.flatten()\n",
    "\n",
    "#     with open(metadata, 'r') as file:\n",
    "#         keyframe_metadata = json.load(file)\n",
    "\n",
    "#     for i, result in enumerate(results):\n",
    "#         print(\n",
    "#             f\"Frame ID: {result['frame_id']}, Similarity: {result['similarity']:.4f}\")\n",
    "\n",
    "#         # Load the image\n",
    "#         frame_path = keyframe_metadata[result['frame_id']][\"frame_path\"]\n",
    "#         print(result['frame_id'])\n",
    "#         image_path = os.path.join(image_dir, frame_path)\n",
    "#         image = cv2.imread(image_path)\n",
    "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#         if visual_encoding:\n",
    "#             image_with_grid = visual_encoding.visualize_grid(image.copy())\n",
    "\n",
    "#             axs[i].imshow(image_with_grid)\n",
    "#             axs[i].set_title(\n",
    "#                 f\"Frame ID: {result['frame_id']}\\nSimilarity: {result['similarity']:.4f}\")\n",
    "#             axs[i].axis('off')\n",
    "#         else:\n",
    "#             axs[i].imshow(image)\n",
    "#             axs[i].set_title(\n",
    "#                 f\"Frame ID: {result['frame_id']}\\nSimilarity: {result['similarity']:.4f}\")\n",
    "#             axs[i].axis('off')\n",
    "\n",
    "#     for j in range(i+1, len(axs)):\n",
    "#         axs[j].axis('off')\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.suptitle(f\"Search Results for Query: '{query}'\", fontsize=16, y=1.02)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# from typing import Optional, Tuple\n",
    "# from open_clip import create_model_and_transforms, get_tokenizer\n",
    "\n",
    "# class OpenClipEmbedder:\n",
    "#     def __init__(self, model_name: str = 'ViT-L-14', pretrained: str = 'datacomp_xl_s13b_b90k', feature_shape: Optional[Tuple[int, ...]] = None):\n",
    "#         # self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#         self.device = \"cpu\"\n",
    "#         self.model, _, _ = create_model_and_transforms(\n",
    "#             model_name, device=self.device, pretrained=pretrained)\n",
    "#         self.model.eval()\n",
    "#         self.tokenizer = get_tokenizer(model_name)\n",
    "#         self.feature_shape = feature_shape\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def embed(self, text: str) -> np.ndarray:\n",
    "#         text_tokens = self.tokenizer([text]).to(self.device)\n",
    "#         text_features = self.model.encode_text(text_tokens)\n",
    "#         embedding = text_features.cpu().numpy()[0]\n",
    "\n",
    "\n",
    "#         resized_embedding = self.resize_embedding(\n",
    "#             embedding, self.feature_shape)\n",
    "\n",
    "\n",
    "#         return resized_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Set up logging configuration.\"\"\"\n",
    "    logging.basicConfig(level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "def validate_input(features_dir):\n",
    "    \"\"\"Validate input directory and find .npy files.\"\"\"\n",
    "    if not os.path.isdir(features_dir):\n",
    "        raise ValueError(f\"Directory not found: {features_dir}\")\n",
    "\n",
    "    npy_files = glob.glob(os.path.join(\n",
    "        features_dir, '**', '*.npy'), recursive=True)\n",
    "    if not npy_files:\n",
    "        raise ValueError(f\"No .npy files found in {features_dir}\")\n",
    "\n",
    "    return npy_files\n",
    "\n",
    "\n",
    "def initialize_index(npy_files, expected_feature_shape):\n",
    "    \"\"\"Initialize FAISS index based on the first feature file.\"\"\"\n",
    "    first_feature = np.load(npy_files[0])\n",
    "    if len(first_feature.shape) != 2:\n",
    "        first_feature = first_feature.reshape(-1, expected_feature_shape)\n",
    "    feature_dim = first_feature.shape[1]\n",
    "\n",
    "    if feature_dim != expected_feature_shape:\n",
    "        logging.warning(\n",
    "            f\"Actual feature dimension {feature_dim} doesn't match expected {expected_feature_shape}\")\n",
    "        logging.info(\n",
    "            f\"Using actual feature dimension {feature_dim} for index creation\")\n",
    "\n",
    "    return faiss.IndexFlatIP(feature_dim), feature_dim\n",
    "\n",
    "\n",
    "def create_gpu_index(cpu_index):\n",
    "    \"\"\"Attempt to create a GPU index.\"\"\"\n",
    "    try:\n",
    "        res = faiss.StandardGpuResources()\n",
    "        gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "        logging.info(\"GPU index creation is available\")\n",
    "        return gpu_index, True\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"GPU index creation not available: {e}\")\n",
    "        return None, False\n",
    "\n",
    "\n",
    "def process_feature_file(feature_path, cpu_index, gpu_index, feature_shape):\n",
    "    \"\"\"Process a single feature file and add to indexes.\"\"\"\n",
    "    try:\n",
    "        feats = np.load(feature_path)\n",
    "        if feats.size == 0:\n",
    "            logging.warning(\n",
    "                f\"Empty array loaded from {feature_path}. Skipping this file.\")\n",
    "            return 0\n",
    "\n",
    "        if len(feats.shape) != 2:\n",
    "            feats = feats.reshape(-1, feature_shape)\n",
    "\n",
    "        feats = feats.astype(np.float32)\n",
    "\n",
    "        if feats.shape[1] != feature_shape:\n",
    "            logging.warning(f\"Feature dimension mismatch in {feature_path}. \"\n",
    "                            f\"Expected {feature_shape}, got {feats.shape[1]}. Skipping this file.\")\n",
    "            return 0\n",
    "\n",
    "        faiss.normalize_L2(feats)\n",
    "\n",
    "        cpu_index.add(feats)\n",
    "        if gpu_index:\n",
    "            gpu_index.add(feats)\n",
    "\n",
    "        return feats.shape[0]\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing {feature_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "def save_indexes(cpu_index, gpu_index, cpu_bin_name, gpu_bin_name, total_vectors):\n",
    "    \"\"\"Save CPU and GPU indexes to disk.\"\"\"\n",
    "    faiss.write_index(cpu_index, cpu_bin_name)\n",
    "    logging.info(\n",
    "        f\"CPU FAISS index with {total_vectors} vectors saved to {cpu_bin_name}\")\n",
    "\n",
    "    if gpu_index:\n",
    "        gpu_index_cpu = faiss.index_gpu_to_cpu(gpu_index)\n",
    "        faiss.write_index(gpu_index_cpu, gpu_bin_name)\n",
    "        logging.info(\n",
    "            f\"GPU FAISS index with {total_vectors} vectors saved to {gpu_bin_name}\")\n",
    "\n",
    "\n",
    "def create_faiss_indexes_clip(cpu_bin_name, gpu_bin_name, features_dir, feature_shape):\n",
    "    \"\"\"\n",
    "    Create both CPU and GPU FAISS indexes for CLIP v2 features.\n",
    "\n",
    "    Parameters:\n",
    "    - cpu_bin_name: Name of the output CPU FAISS index file\n",
    "    - gpu_bin_name: Name of the output GPU FAISS index file\n",
    "    - features_dir: Directory containing feature files\n",
    "    - feature_shape: Expected shape of each feature vector\n",
    "\n",
    "    Returns:\n",
    "    - None (saves the indexes to disk)\n",
    "    \"\"\"\n",
    "    setup_logging()\n",
    "    npy_files = validate_input(features_dir)\n",
    "    cpu_index, feature_dim = initialize_index(npy_files, feature_shape)\n",
    "    gpu_index, use_gpu = create_gpu_index(cpu_index)\n",
    "\n",
    "    total_vectors = 0\n",
    "    with tqdm(total=len(npy_files), desc=\"Processing feature files\", unit=\"file\") as pbar:\n",
    "        for feature_path in npy_files:\n",
    "            vectors_added = process_feature_file(\n",
    "                feature_path, cpu_index, gpu_index, feature_dim)\n",
    "            total_vectors += vectors_added\n",
    "            pbar.update(1)\n",
    "            pbar.set_postfix({'Total Vectors': total_vectors})\n",
    "\n",
    "    save_indexes(cpu_index, gpu_index, cpu_bin_name,\n",
    "                 gpu_bin_name, total_vectors)\n",
    "    logging.info(\"Indexing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-08 11:42:29,934 - WARNING - Actual feature dimension 768 doesn't match expected 512\n",
      "2024-09-08 11:42:29,935 - INFO - Using actual feature dimension 768 for index creation\n",
      "2024-09-08 11:42:29,936 - WARNING - GPU index creation not available: module 'faiss' has no attribute 'StandardGpuResources'\n",
      "Processing feature files: 100%|██████████| 4/4 [00:00<00:00, 142.51file/s, Total Vectors=2132]\n",
      "2024-09-08 11:42:29,974 - INFO - CPU FAISS index with 2132 vectors saved to faiss_clipv2_cosine_cpu.bin\n",
      "2024-09-08 11:42:29,975 - INFO - Indexing complete.\n"
     ]
    }
   ],
   "source": [
    "create_faiss_indexes_clip(cpu_bin_name, gpu_bin_name, features_dir, feature_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = faiss.read_index(cpu_bin_name)\n",
    "\n",
    "# vectorizer = OpenClipEmbedder()\n",
    "# query = \"the policeman\"\n",
    "# results = search_similar_frames(query, index, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(vector_path):\n",
    "    for item in os.listdir(vector_path):\n",
    "        if item.split('_')[-1] == \"vectors.npz\":\n",
    "            # Load the vectors from the .npz file\n",
    "            vectors = load_npz(f\"{vector_path}/{item}\")\n",
    "    \n",
    "            # Convert to dense numpy array if it's a sparse matrix\n",
    "            if isinstance(vectors, np.ndarray):\n",
    "                return vectors.astype('float32')\n",
    "            else:\n",
    "                return vectors.toarray().astype('float32')\n",
    "            \n",
    "def create_and_save_faiss_index(vector_path, output_path):\n",
    "    # Load vectors\n",
    "    vectors = load_vectors(vector_path)\n",
    "    \n",
    "    # Create FAISS index\n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(vectors)\n",
    "    \n",
    "    # Save the index\n",
    "    faiss.write_index(index, output_path)\n",
    "    print(f\"FAISS index saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_path = f\"{metadata_encoded_path}/ocr\"\n",
    "output_path = f\"{dir_path}/{ocr_bin_name}\"\n",
    "\n",
    "# create_and_save_faiss_index(vector_path, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing multi-tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vectors(vector_path):\n",
    "    for item in os.listdir(vector_path):\n",
    "        if item.split('_')[-1] == \"vectors.npz\":\n",
    "            # Load the vectors from the .npz file\n",
    "            vectors = load_npz(f\"{vector_path}/{item}\")\n",
    "    \n",
    "            # Convert to dense numpy array if it's a sparse matrix\n",
    "            if isinstance(vectors, np.ndarray):\n",
    "                return vectors.astype('float32')\n",
    "            else:\n",
    "                return vectors.toarray().astype('float32')\n",
    "            \n",
    "# def create_faiss_index(vectors, nlist=100):\n",
    "#     # vectors = vectors.astype('float32').toarray()\n",
    "#     dimension = vectors.shape[1]\n",
    "    \n",
    "#     quantizer = faiss.IndexFlatIP(dimension)\n",
    "#     index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n",
    "    \n",
    "#     faiss.normalize_L2(vectors)\n",
    "    \n",
    "#     index.train(vectors)\n",
    "#     index.add(vectors)\n",
    "#     return index\n",
    "\n",
    "def create_faiss_index_flat(vectors, output_path):\n",
    "    # vectors = vectors.astype('float32').toarray()\n",
    "    faiss.normalize_L2(vectors)\n",
    "    \n",
    "    dimension = vectors.shape[1]\n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    index.add(vectors)\n",
    "    \n",
    "    # Save the index    \n",
    "    faiss.write_index(index, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_npz' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m vector_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetadata_encoded_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/multi_tag\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdir_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmulti_tag_bin_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m vectors \u001b[38;5;241m=\u001b[39m \u001b[43mload_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# create_faiss_index_flat(vectors, output_path)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m, in \u001b[0;36mload_vectors\u001b[0;34m(vector_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(vector_path):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvectors.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# Load the vectors from the .npz file\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m         vectors \u001b[38;5;241m=\u001b[39m \u001b[43mload_npz\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvector_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# Convert to dense numpy array if it's a sparse matrix\u001b[39;00m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(vectors, np\u001b[38;5;241m.\u001b[39mndarray):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_npz' is not defined"
     ]
    }
   ],
   "source": [
    "# Create and save FAISS index\n",
    "vector_path = f\"{metadata_encoded_path}/multi_tag\"\n",
    "output_path = f\"{dir_path}/{multi_tag_bin_name}\"\n",
    "\n",
    "vectors = load_vectors(vector_path)\n",
    "# create_faiss_index_flat(vectors, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
