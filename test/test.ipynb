{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fuduweiii/IT/miniconda3/envs/hcm-ai/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import os\n",
    "\n",
    "# Load CLIP model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def embed_text_query(query):\n",
    "    \"\"\"\n",
    "    Embed a text query using CLIP.\n",
    "    \n",
    "    Args:\n",
    "    query (str): The text query to embed.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: The embedding of the text query.\n",
    "    \"\"\"\n",
    "    inputs = processor(text=[query], return_tensors=\"pt\", padding=True)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**inputs)\n",
    "    return text_features.numpy()\n",
    "\n",
    "def get_image_ids(folder_path):\n",
    "    \"\"\"\n",
    "    Generate image IDs based on the folder structure: Keyframes_L01/keyframes/L01_V001/001.jpg\n",
    "    \n",
    "    Args:\n",
    "    folder_path (str): Path to the folder containing keyframe subfolders (e.g., path to Keyframes_L01/keyframes).\n",
    "    \n",
    "    Returns:\n",
    "    list: A list of image IDs.\n",
    "    \"\"\"\n",
    "    image_ids = []\n",
    "    for video_folder in sorted(os.listdir(folder_path)):\n",
    "        video_path = os.path.join(folder_path, video_folder)\n",
    "        if os.path.isdir(video_path):\n",
    "            for file in sorted(os.listdir(video_path)):\n",
    "                if file.endswith('.jpg') or file.endswith('.png'):\n",
    "                    # Create image ID in the format L01_V001_001\n",
    "                    image_id = f\"{video_folder}_{os.path.splitext(file)[0]}\"\n",
    "                    image_ids.append(image_id)\n",
    "    return image_ids\n",
    "\n",
    "def load_image_embeddings(embeddings_folder):\n",
    "    \"\"\"\n",
    "    Load image embeddings from .npy files.\n",
    "    \n",
    "    Args:\n",
    "    embeddings_folder (str): Path to the folder containing .npy files.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: A 2D array of image embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for file in sorted(os.listdir(embeddings_folder)):\n",
    "        if file.endswith('.npy'):\n",
    "            embedding = np.load(os.path.join(embeddings_folder, file))\n",
    "            embeddings.append(embedding)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "text_query_embedding = embed_text_query(\"a lot of grapefruits with a sign that says 'Bưởi Tân Triều, đệ nhất bưởi Đồng Nai'\")\n",
    "image_embeddings = load_image_embeddings('/home/fuduweiii/IT/Project/python/pipeline-hcm-ai/test/clip-features-32-b1/clip-features-32')\n",
    "image_ids = get_image_ids('/home/fuduweiii/IT/Project/python/pipeline-hcm-ai/test/Keyframes_L01/keyframes')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Retrieval Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def initial_retrieval(text_query_embedding, image_embeddings, top_k=1000):\n",
    "    \"\"\"\n",
    "    Perform initial retrieval of top-k most similar images to the text query.\n",
    "    \n",
    "    Args:\n",
    "    text_query_embedding (np.array): Embedding of the text query (1, D)\n",
    "    image_embeddings (np.array): Embeddings of all images in the database (N, D)\n",
    "    top_k (int): Number of top results to retrieve\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (indices of top-k images, similarities of top-k images, embeddings of top-k images)\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity(text_query_embedding.reshape(1, -1), image_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[-top_k:][::-1]\n",
    "    return top_k_indices, similarities[top_k_indices], image_embeddings[top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Nearest Neighbors Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_neighbors(query_embedding, database_embeddings, k=5):\n",
    "    \"\"\"\n",
    "    Find k nearest neighbors for a given query embedding.\n",
    "    \n",
    "    Args:\n",
    "    query_embedding (np.array): Embedding of the query image (1, D)\n",
    "    database_embeddings (np.array): Embeddings to search in (N, D)\n",
    "    k (int): Number of nearest neighbors to find\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (indices of k nearest neighbors, similarities of k nearest neighbors)\n",
    "    \"\"\"\n",
    "    similarities = cosine_similarity(query_embedding.reshape(1, -1), database_embeddings)[0]\n",
    "    top_k_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    return top_k_indices, similarities[top_k_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted Average Pooling Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average_pooling(embeddings, similarities, beta=0.15):\n",
    "    \"\"\"\n",
    "    Perform weighted average pooling on embeddings.\n",
    "    \n",
    "    Args:\n",
    "    embeddings (np.array): Embeddings to pool (N, D)\n",
    "    similarities (np.array): Similarity scores for weighting (N,)\n",
    "    beta (float): Exponent for similarity scores\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Weighted average embedding (D,)\n",
    "    \"\"\"\n",
    "    weights = similarities ** beta\n",
    "    weighted_sum = np.sum(embeddings * weights[:, np.newaxis], axis=0)\n",
    "    return weighted_sum / np.sum(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refine Embeddings Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_embeddings(top_k_embeddings, neighbors=5, beta=0.15):\n",
    "    \"\"\"\n",
    "    Refine the top-k embeddings using their nearest neighbors.\n",
    "    \n",
    "    Args:\n",
    "    top_k_embeddings (np.array): Embeddings of top-k images (K, D)\n",
    "    neighbors (int): Number of nearest neighbors to consider\n",
    "    beta (float): Exponent for similarity scores in weighted average pooling\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Refined embeddings (K, D)\n",
    "    \"\"\"\n",
    "    refined_embeddings = []\n",
    "    for embedding in top_k_embeddings:\n",
    "        neighbor_indices, neighbor_similarities = find_nearest_neighbors(embedding, top_k_embeddings, neighbors)\n",
    "        neighbor_embeddings = top_k_embeddings[neighbor_indices]\n",
    "        refined_embedding = weighted_average_pooling(neighbor_embeddings, neighbor_similarities, beta)\n",
    "        refined_embeddings.append(refined_embedding)\n",
    "    return np.array(refined_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expand Query Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(text_query_embedding, top_k_embeddings, neighbors=5):\n",
    "    \"\"\"\n",
    "    Expand the text query embedding using top similar image embeddings.\n",
    "    \n",
    "    Args:\n",
    "    text_query_embedding (np.array): Original text query embedding (1, D)\n",
    "    top_k_embeddings (np.array): Embeddings of top-k images (K, D)\n",
    "    neighbors (int): Number of nearest neighbors to consider\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Expanded query embedding (D,)\n",
    "    \"\"\"\n",
    "    query_neighbors = find_nearest_neighbors(text_query_embedding, top_k_embeddings, neighbors)[0]\n",
    "    return np.max(top_k_embeddings[query_neighbors], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Final Scores Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_final_scores(text_query_embedding, refined_embeddings, original_embeddings, expanded_query):\n",
    "    \"\"\"\n",
    "    Compute final similarity scores for reranking.\n",
    "    \n",
    "    Args:\n",
    "    text_query_embedding (np.array): Original text query embedding (1, D)\n",
    "    refined_embeddings (np.array): Refined embeddings of top-k images (K, D)\n",
    "    original_embeddings (np.array): Original embeddings of top-k images (K, D)\n",
    "    expanded_query (np.array): Expanded query embedding (D,)\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Final similarity scores (K,)\n",
    "    \"\"\"\n",
    "    s1 = cosine_similarity(text_query_embedding, refined_embeddings)[0]\n",
    "    s2 = cosine_similarity(expanded_query.reshape(1, -1), original_embeddings)[0]\n",
    "    return (s1 + s2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SuperGlobal Reranking Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def superglobal_reranking(text_query_embedding, image_embeddings, image_ids, top_k=1000, neighbors=5, beta=0.15):\n",
    "    \"\"\"\n",
    "    Perform SuperGlobal Reranking for a text query.\n",
    "    \n",
    "    Args:\n",
    "    text_query_embedding (np.array): Embedding of the text query (1, D)\n",
    "    image_embeddings (np.array): Embeddings of all images in the database (N, D)\n",
    "    image_ids (np.array): IDs of all images in the database (N,)\n",
    "    top_k (int): Number of top results to retrieve and rerank\n",
    "    neighbors (int): Number of nearest neighbors to consider\n",
    "    beta (float): Exponent for similarity scores in weighted average pooling\n",
    "    \n",
    "    Returns:\n",
    "    np.array: Reranked image IDs\n",
    "    \"\"\"\n",
    "    # Initial retrieval\n",
    "    top_k_indices, top_k_similarities, top_k_embeddings = initial_retrieval(text_query_embedding, image_embeddings, top_k)\n",
    "    \n",
    "    # Refine embeddings\n",
    "    refined_embeddings = refine_embeddings(top_k_embeddings, neighbors, beta)\n",
    "    \n",
    "    # Expand query\n",
    "    expanded_query = expand_query(text_query_embedding, top_k_embeddings, neighbors)\n",
    "    \n",
    "    # Compute final scores\n",
    "    final_scores = compute_final_scores(text_query_embedding, refined_embeddings, top_k_embeddings, expanded_query)\n",
    "    \n",
    "    # Rerank\n",
    "    reranked_indices = np.argsort(final_scores)[::-1]\n",
    "    reranked_image_ids = image_ids[top_k_indices[reranked_indices]]\n",
    "    \n",
    "    return reranked_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming you have these variables prepared:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# text_query_embedding: the CLIP embedding of your text query\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# image_embeddings: CLIP embeddings of all images in your database\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# image_ids: corresponding IDs for all images\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m reranked_ids \u001b[38;5;241m=\u001b[39m \u001b[43msuperglobal_reranking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_query_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# The reranked_ids will contain the IDs of the images, ordered by relevance after reranking\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 30\u001b[0m, in \u001b[0;36msuperglobal_reranking\u001b[0;34m(text_query_embedding, image_embeddings, image_ids, top_k, neighbors, beta)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Rerank\u001b[39;00m\n\u001b[1;32m     29\u001b[0m reranked_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(final_scores)[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m reranked_image_ids \u001b[38;5;241m=\u001b[39m \u001b[43mimage_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtop_k_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[43mreranked_indices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reranked_image_ids\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# Assuming you have these variables prepared:\n",
    "# text_query_embedding: the CLIP embedding of your text query\n",
    "# image_embeddings: CLIP embeddings of all images in your database\n",
    "# image_ids: corresponding IDs for all images\n",
    "\n",
    "reranked_ids = superglobal_reranking(text_query_embedding, image_embeddings, image_ids)\n",
    "\n",
    "# The reranked_ids will contain the IDs of the images, ordered by relevance after reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
